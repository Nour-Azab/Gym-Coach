{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4949f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc2b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data prepared.\n",
      "  phase  phase_label\n",
      "0    S1            0\n",
      "1    S1            0\n",
      "2    S1            0\n",
      "3    S1            0\n",
      "4    S1            0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"pose_landmarks_all.csv\")\n",
    "\n",
    "df = df.sort_values(by=[\"video_name\", \"frame\"]).reset_index(drop=True)\n",
    "\n",
    "meta_cols = [\"frame\", \"video_name\", \"rep_counter\", \"phase\"]\n",
    "pose_cols = [c for c in df.columns if c not in meta_cols]\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"phase_label\"] = label_encoder.fit_transform(df[\"phase\"])\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "df[pose_cols] = scaler.fit_transform(df[pose_cols])\n",
    "\n",
    "print(df[[\"phase\", \"phase_label\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e451747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created windowed data: X=(2283, 30, 99), y=(2283,)\n"
     ]
    }
   ],
   "source": [
    "def create_windows_for_video(data, labels, window_size=30, stride=5):\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(data) - window_size, stride):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(labels[i+window_size//2])  # get the middle frame of th window\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_all, y_all = [], []\n",
    "\n",
    "# group by videos so windows doesn't overlap between videos\n",
    "for _, group in df.groupby(\"video_name\"):\n",
    "    X_tmp, y_tmp = create_windows_for_video(\n",
    "        group[pose_cols].values,\n",
    "        group[\"phase_label\"].values,\n",
    "        window_size=30,\n",
    "        stride=5\n",
    "    )\n",
    "    X_all.append(X_tmp)\n",
    "    y_all.append(y_tmp)\n",
    "\n",
    "X_all = np.concatenate(X_all, axis=0)\n",
    "y_all = np.concatenate(y_all, axis=0)\n",
    "\n",
    "print(f\"Size of windowed data: X={X_all.shape}, y={y_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e4be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1598, 30, 99), Val: (342, 30, 99), Test: (343, 30, 99)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_all, y_all, test_size=0.3, random_state=42, stratify=y_all)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_t   = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_t   = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_t  = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Dataloaders\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "val_ds = TensorDataset(X_val_t, y_val_t)\n",
    "test_ds = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)# shuffle is good for training so model does't see similar consecutive frames every epoch\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25a325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAutoencoderClassifier(nn.Module):\n",
    "    def __init__(self, num_features, seq_len, d_model=128, nhead=8, num_layers=3, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Encoder Layer\n",
    "        self.input_proj = nn.Linear(num_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Decoder for reconstruction\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.output_proj = nn.Linear(d_model, num_features)\n",
    "\n",
    "        # Classification Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(d_model * seq_len, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.input_proj(x)\n",
    "        memory = self.encoder(z)\n",
    "        reconstructed = self.decoder(z, memory)\n",
    "        recon_out = self.output_proj(reconstructed)\n",
    "        class_out = self.classifier(memory)\n",
    "        return recon_out, class_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d61a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X_train.shape[1]\n",
    "num_features = X_train.shape[2]\n",
    "num_classes = len(np.unique(y_all))\n",
    "\n",
    "model = TransformerAutoencoderClassifier(num_features, seq_len, num_classes=num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "mse_loss = nn.MSELoss()# for reconstruction loss\n",
    "ce_loss = nn.CrossEntropyLoss()# for classification loss\n",
    "\n",
    "epochs = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d4be0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] | Train Loss: 0.9498 | Val Loss: 0.5377 | Train Acc: 0.714 | Val Acc: 0.825\n",
      "Epoch [2/30] | Train Loss: 0.4724 | Val Loss: 0.3794 | Train Acc: 0.844 | Val Acc: 0.877\n",
      "Epoch [3/30] | Train Loss: 0.3441 | Val Loss: 0.2903 | Train Acc: 0.880 | Val Acc: 0.880\n",
      "Epoch [4/30] | Train Loss: 0.2895 | Val Loss: 0.2608 | Train Acc: 0.888 | Val Acc: 0.889\n",
      "Epoch [5/30] | Train Loss: 0.2283 | Val Loss: 0.2201 | Train Acc: 0.916 | Val Acc: 0.889\n",
      "Epoch [6/30] | Train Loss: 0.1939 | Val Loss: 0.1764 | Train Acc: 0.931 | Val Acc: 0.921\n",
      "Epoch [7/30] | Train Loss: 0.1800 | Val Loss: 0.1744 | Train Acc: 0.936 | Val Acc: 0.912\n",
      "Epoch [8/30] | Train Loss: 0.1608 | Val Loss: 0.2074 | Train Acc: 0.937 | Val Acc: 0.883\n",
      "Epoch [9/30] | Train Loss: 0.1513 | Val Loss: 0.1536 | Train Acc: 0.934 | Val Acc: 0.915\n",
      "Epoch [10/30] | Train Loss: 0.1332 | Val Loss: 0.1814 | Train Acc: 0.942 | Val Acc: 0.898\n",
      "Epoch [11/30] | Train Loss: 0.1363 | Val Loss: 0.1337 | Train Acc: 0.933 | Val Acc: 0.936\n",
      "Epoch [12/30] | Train Loss: 0.1267 | Val Loss: 0.1438 | Train Acc: 0.937 | Val Acc: 0.904\n",
      "Epoch [13/30] | Train Loss: 0.1142 | Val Loss: 0.1141 | Train Acc: 0.942 | Val Acc: 0.942\n",
      "Epoch [14/30] | Train Loss: 0.1025 | Val Loss: 0.1403 | Train Acc: 0.947 | Val Acc: 0.912\n",
      "Epoch [15/30] | Train Loss: 0.1047 | Val Loss: 0.1068 | Train Acc: 0.946 | Val Acc: 0.936\n",
      "Epoch [16/30] | Train Loss: 0.0887 | Val Loss: 0.1206 | Train Acc: 0.956 | Val Acc: 0.930\n",
      "Epoch [17/30] | Train Loss: 0.0885 | Val Loss: 0.1113 | Train Acc: 0.947 | Val Acc: 0.939\n",
      "Epoch [18/30] | Train Loss: 0.0803 | Val Loss: 0.1066 | Train Acc: 0.965 | Val Acc: 0.933\n",
      "Epoch [19/30] | Train Loss: 0.0841 | Val Loss: 0.1112 | Train Acc: 0.949 | Val Acc: 0.930\n",
      "Epoch [20/30] | Train Loss: 0.0799 | Val Loss: 0.0914 | Train Acc: 0.956 | Val Acc: 0.953\n",
      "Epoch [21/30] | Train Loss: 0.0703 | Val Loss: 0.1222 | Train Acc: 0.963 | Val Acc: 0.927\n",
      "Epoch [22/30] | Train Loss: 0.0715 | Val Loss: 0.0973 | Train Acc: 0.961 | Val Acc: 0.936\n",
      "Epoch [23/30] | Train Loss: 0.0705 | Val Loss: 0.0912 | Train Acc: 0.955 | Val Acc: 0.950\n",
      "Epoch [24/30] | Train Loss: 0.0624 | Val Loss: 0.1045 | Train Acc: 0.967 | Val Acc: 0.930\n",
      "Epoch [25/30] | Train Loss: 0.0640 | Val Loss: 0.1042 | Train Acc: 0.966 | Val Acc: 0.939\n",
      "Epoch [26/30] | Train Loss: 0.0678 | Val Loss: 0.0976 | Train Acc: 0.959 | Val Acc: 0.936\n",
      "Epoch [27/30] | Train Loss: 0.0687 | Val Loss: 0.0929 | Train Acc: 0.959 | Val Acc: 0.939\n",
      "Epoch [28/30] | Train Loss: 0.0677 | Val Loss: 0.0804 | Train Acc: 0.958 | Val Acc: 0.947\n",
      "Epoch [29/30] | Train Loss: 0.0575 | Val Loss: 0.1077 | Train Acc: 0.968 | Val Acc: 0.936\n",
      "Epoch [30/30] | Train Loss: 0.0539 | Val Loss: 0.0939 | Train Acc: 0.971 | Val Acc: 0.944\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss, train_correct, total = 0, 0, 0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_out, class_out = model(x_batch)\n",
    "\n",
    "        loss_recon = mse_loss(recon_out, x_batch)\n",
    "        loss_class = ce_loss(class_out, y_batch)\n",
    "        loss = loss_recon + 0.5 * loss_class# mix between losses to give more importance for each job that has higher loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(class_out, dim=1)\n",
    "        train_correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    train_acc = train_correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            recon_out, class_out = model(x_val)\n",
    "            loss_recon = mse_loss(recon_out, x_val)\n",
    "            loss_class = ce_loss(class_out, y_val)\n",
    "            loss = loss_recon + 0.5 * loss_class\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(class_out, dim=1)\n",
    "            val_correct += (preds == y_val).sum().item()\n",
    "            val_total += y_val.size(0)\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {train_loss/len(train_loader):.4f} | \"\n",
    "          f\"Val Loss: {val_loss/len(val_loader):.4f} | Train Acc: {train_acc:.3f} | Val Acc: {val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce13a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test Accuracy: 0.9533527696793003\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          S1       0.96      0.98      0.97       163\n",
      "          S2       0.90      0.92      0.91        49\n",
      "          S3       0.96      0.98      0.97       104\n",
      "          S4       0.95      0.78      0.86        27\n",
      "\n",
      "    accuracy                           0.95       343\n",
      "   macro avg       0.95      0.91      0.93       343\n",
      "weighted avg       0.95      0.95      0.95       343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        _, class_out = model(x_batch)\n",
    "        preds = torch.argmax(class_out, dim=1).cpu().numpy()\n",
    "        y_true.extend(y_batch.numpy())\n",
    "        y_pred.extend(preds)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2be16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and preprocessors saved.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"transformer_multi_task.pth\")\n",
    "joblib.dump(scaler, \"pose_scaler.pkl\")\n",
    "joblib.dump(label_encoder, \"phase_encoder.pkl\")\n",
    "print(\"Model and preprocessors saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
