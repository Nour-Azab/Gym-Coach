{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4949f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27ca988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"\n",
    "    Calculates the angle in degrees between three points (a, b, c),\n",
    "    where 'b' is the vertex.\n",
    "    \"\"\"\n",
    "    a, b, c = np.array(a), np.array(b), np.array(c)\n",
    "    ba, bc = a - b, c - b\n",
    "    dot_product = np.dot(ba, bc)\n",
    "    norm_ba, norm_bc = np.linalg.norm(ba), np.linalg.norm(bc)\n",
    "    if norm_ba == 0 or norm_bc == 0:\n",
    "        return 0.0\n",
    "    cosine_angle = np.clip(dot_product / (norm_ba * norm_bc), -1.0, 1.0)\n",
    "    return np.degrees(np.arccos(cosine_angle))\n",
    "\n",
    "\n",
    "def add_pose_angles(df):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "\n",
    "    left_knee_angles, right_knee_angles = [], []\n",
    "    left_hip_angles, right_hip_angles = [], []\n",
    "    left_torso_angles, right_torso_angles = [], []\n",
    "    left_ankle_angles, right_ankle_angles = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # --- Left landmarks ---\n",
    "        left_shoulder = [row['LEFT_SHOULDER_x'], row['LEFT_SHOULDER_y']]\n",
    "        left_hip = [row['LEFT_HIP_x'], row['LEFT_HIP_y']]\n",
    "        left_knee = [row['LEFT_KNEE_x'], row['LEFT_KNEE_y']]\n",
    "        left_ankle = [row['LEFT_ANKLE_x'], row['LEFT_ANKLE_y']]\n",
    "        left_foot_index = [row['LEFT_FOOT_INDEX_x'], row['LEFT_FOOT_INDEX_y']]\n",
    "\n",
    "        # --- Right landmarks ---\n",
    "        right_shoulder = [row['RIGHT_SHOULDER_x'], row['RIGHT_SHOULDER_y']]\n",
    "        right_hip = [row['RIGHT_HIP_x'], row['RIGHT_HIP_y']]\n",
    "        right_knee = [row['RIGHT_KNEE_x'], row['RIGHT_KNEE_y']]\n",
    "        right_ankle = [row['RIGHT_ANKLE_x'], row['RIGHT_ANKLE_y']]\n",
    "        right_foot_index = [row['RIGHT_FOOT_INDEX_x'], row['RIGHT_FOOT_INDEX_y']]\n",
    "\n",
    "        # --- Calculate angles ---\n",
    "        left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
    "        right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
    "        left_hip_angle = calculate_angle(left_shoulder, left_hip, left_knee)\n",
    "        right_hip_angle = calculate_angle(right_shoulder, right_hip, right_knee)\n",
    "\n",
    "        left_hip_vertical = [left_hip[0], left_hip[1] - 1]\n",
    "        right_hip_vertical = [right_hip[0], right_hip[1] - 1]\n",
    "        left_torso_angle = calculate_angle(left_shoulder, left_hip, left_hip_vertical)\n",
    "        right_torso_angle = calculate_angle(right_shoulder, right_hip, right_hip_vertical)\n",
    "\n",
    "        left_ankle_angle = calculate_angle(left_knee, left_ankle, left_foot_index)\n",
    "        right_ankle_angle = calculate_angle(right_knee, right_ankle, right_foot_index)\n",
    "\n",
    "        # --- Append to lists ---\n",
    "        left_knee_angles.append(left_knee_angle)\n",
    "        right_knee_angles.append(right_knee_angle)\n",
    "        left_hip_angles.append(left_hip_angle)\n",
    "        right_hip_angles.append(right_hip_angle)\n",
    "        left_torso_angles.append(left_torso_angle)\n",
    "        right_torso_angles.append(right_torso_angle)\n",
    "        left_ankle_angles.append(left_ankle_angle)\n",
    "        right_ankle_angles.append(right_ankle_angle)\n",
    "\n",
    "    # --- Add new columns ---\n",
    "    df['left_knee_angle'] = left_knee_angles\n",
    "    df['right_knee_angle'] = right_knee_angles\n",
    "    df['left_hip_angle'] = left_hip_angles\n",
    "    df['right_hip_angle'] = right_hip_angles\n",
    "    df['left_torso_angle'] = left_torso_angles\n",
    "    df['right_torso_angle'] = right_torso_angles\n",
    "    df['left_ankle_angle'] = left_ankle_angles\n",
    "    df['right_ankle_angle'] = right_ankle_angles\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fdc2b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   frame   video_name  rep_counter  NOSE_x  NOSE_y  NOSE_visibility  \\\n",
      "0      1  plus233.mp4            0     234      87         0.999974   \n",
      "1      2  plus233.mp4            0     234      84         0.999972   \n",
      "2      3  plus233.mp4            0     233      82         0.999969   \n",
      "3      4  plus233.mp4            0     233      82         0.999968   \n",
      "4      5  plus233.mp4            0     235      82         0.999968   \n",
      "\n",
      "   LEFT_EYE_INNER_x  LEFT_EYE_INNER_y  LEFT_EYE_INNER_visibility  LEFT_EYE_x  \\\n",
      "0               231                77                   0.999970         231   \n",
      "1               230                74                   0.999968         230   \n",
      "2               229                72                   0.999967         229   \n",
      "3               229                73                   0.999966         229   \n",
      "4               231                73                   0.999965         231   \n",
      "\n",
      "   ...  RIGHT_FOOT_INDEX_visibility  phase  left_knee_angle  right_knee_angle  \\\n",
      "0  ...                     0.988252     S1       175.175000        179.950161   \n",
      "1  ...                     0.986794     S1       174.963153        179.697224   \n",
      "2  ...                     0.986186     S1       174.989917        179.329734   \n",
      "3  ...                     0.984570     S1       176.769375        178.613574   \n",
      "4  ...                     0.983513     S1       177.601209        178.191772   \n",
      "\n",
      "   left_hip_angle  right_hip_angle  left_torso_angle  right_torso_angle  \\\n",
      "0      176.726538       168.479375          0.651060           3.503532   \n",
      "1      175.962261       167.968458          0.333111           3.955645   \n",
      "2      175.885091       167.968458          0.000000           3.955645   \n",
      "3      175.986702       167.362296          0.335059           4.561808   \n",
      "4      176.367011       167.665258          0.339024           4.258846   \n",
      "\n",
      "   left_ankle_angle  right_ankle_angle  \n",
      "0         93.087483         132.563751  \n",
      "1        104.856986         125.189231  \n",
      "2        107.009274         121.314622  \n",
      "3        109.146597         117.212027  \n",
      "4        111.451219         116.985774  \n",
      "\n",
      "[5 rows x 111 columns]\n",
      "  phase  phase_label\n",
      "0    S1            0\n",
      "1    S3            2\n",
      "2    S3            2\n",
      "3    S4            3\n",
      "4    S2            1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"last_csv_all.csv\")\n",
    "df = add_pose_angles(df)\n",
    "print(df.head())\n",
    "df = df.sort_values(by=[\"video_name\", \"frame\"]).reset_index(drop=True)\n",
    "\n",
    "df.drop(columns=['NOSE_x','NOSE_y','NOSE_visibility','LEFT_EYE_INNER_x','LEFT_EYE_INNER_y','LEFT_EYE_INNER_visibility','LEFT_EYE_x','LEFT_EYE_y','LEFT_EYE_visibility','LEFT_EYE_OUTER_x','LEFT_EYE_OUTER_y','LEFT_EYE_OUTER_visibility','RIGHT_EYE_INNER_x','RIGHT_EYE_INNER_y','RIGHT_EYE_INNER_visibility','RIGHT_EYE_x','RIGHT_EYE_y','RIGHT_EYE_visibility','RIGHT_EYE_OUTER_x','RIGHT_EYE_OUTER_y','RIGHT_EYE_OUTER_visibility','LEFT_EAR_x','LEFT_EAR_y','LEFT_EAR_visibility','RIGHT_EAR_x','RIGHT_EAR_y','RIGHT_EAR_visibility','MOUTH_LEFT_x','MOUTH_LEFT_y','MOUTH_LEFT_visibility','MOUTH_RIGHT_x','MOUTH_RIGHT_y','MOUTH_RIGHT_visibility'],inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "meta_cols = [\"frame\", \"video_name\", \"rep_counter\", \"phase\"]\n",
    "pose_cols = [c for c in df.columns if c not in meta_cols]\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"phase_label\"] = label_encoder.fit_transform(df[\"phase\"])\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "df[pose_cols] = scaler.fit_transform(df[pose_cols])\n",
    "\n",
    "print(df[[\"phase\", \"phase_label\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e451747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of windowed data: X=(7046, 30, 74), y=(7046,)\n"
     ]
    }
   ],
   "source": [
    "def create_windows_for_video(data, labels, window_size=30, stride=5):\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(data) - window_size, stride):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(labels[i+window_size//2])  # get the middle frame of th window\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_all, y_all = [], []\n",
    "\n",
    "# group by videos so windows doesn't overlap between videos\n",
    "for _, group in df.groupby(\"video_name\"):\n",
    "    X_tmp, y_tmp = create_windows_for_video(\n",
    "        group[pose_cols].values,\n",
    "        group[\"phase_label\"].values,\n",
    "        window_size=30,\n",
    "        stride=5\n",
    "    )\n",
    "    X_all.append(X_tmp)\n",
    "    y_all.append(y_tmp)\n",
    "\n",
    "X_all = np.concatenate(X_all, axis=0)\n",
    "y_all = np.concatenate(y_all, axis=0)\n",
    "\n",
    "print(f\"Size of windowed data: X={X_all.shape}, y={y_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "312e4be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (5636, 30, 74), Val: (705, 30, 74), Test: (705, 30, 74)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_all, y_all, test_size=0.2, random_state=42, stratify=y_all)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_t   = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_t   = torch.tensor(y_val, dtype=torch.long)\n",
    "X_test_t  = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Dataloaders\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "val_ds = TensorDataset(X_val_t, y_val_t)\n",
    "test_ds = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)# shuffle is good for training so model does't see similar consecutive frames every epoch\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc25a325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAutoencoderClassifier(nn.Module):\n",
    "    def __init__(self, num_features, seq_len, d_model=128, nhead=8, num_layers=3, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Encoder Layer\n",
    "        self.input_proj = nn.Linear(num_features, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Decoder for reconstruction\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.output_proj = nn.Linear(d_model, num_features)\n",
    "\n",
    "        # Classification Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(d_model * seq_len, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.input_proj(x)\n",
    "        memory = self.encoder(z)\n",
    "        reconstructed = self.decoder(z, memory)\n",
    "        recon_out = self.output_proj(reconstructed)\n",
    "        class_out = self.classifier(memory)\n",
    "        return recon_out, class_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d61a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X_train.shape[1]\n",
    "num_features = X_train.shape[2]\n",
    "num_classes = len(np.unique(y_all))\n",
    "\n",
    "model = TransformerAutoencoderClassifier(num_features, seq_len, num_classes=num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "mse_loss = nn.MSELoss()# for reconstruction loss\n",
    "ce_loss = nn.CrossEntropyLoss()# for classification loss\n",
    "\n",
    "epochs = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1d4be0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30] | Train Loss: 0.5543 | Val Loss: 0.3370 | Train Acc: 0.845 | Val Acc: 0.877\n",
      "Epoch [2/30] | Train Loss: 0.2747 | Val Loss: 0.2458 | Train Acc: 0.894 | Val Acc: 0.906\n",
      "Epoch [3/30] | Train Loss: 0.1988 | Val Loss: 0.3053 | Train Acc: 0.909 | Val Acc: 0.870\n",
      "Epoch [4/30] | Train Loss: 0.1668 | Val Loss: 0.1722 | Train Acc: 0.918 | Val Acc: 0.919\n",
      "Epoch [5/30] | Train Loss: 0.1463 | Val Loss: 0.1724 | Train Acc: 0.921 | Val Acc: 0.913\n",
      "Epoch [6/30] | Train Loss: 0.1284 | Val Loss: 0.1771 | Train Acc: 0.929 | Val Acc: 0.913\n",
      "Epoch [7/30] | Train Loss: 0.1215 | Val Loss: 0.1581 | Train Acc: 0.930 | Val Acc: 0.925\n",
      "Epoch [8/30] | Train Loss: 0.1090 | Val Loss: 0.1429 | Train Acc: 0.940 | Val Acc: 0.922\n",
      "Epoch [9/30] | Train Loss: 0.1055 | Val Loss: 0.1738 | Train Acc: 0.939 | Val Acc: 0.912\n",
      "Epoch [10/30] | Train Loss: 0.0992 | Val Loss: 0.1450 | Train Acc: 0.943 | Val Acc: 0.926\n",
      "Epoch [11/30] | Train Loss: 0.0903 | Val Loss: 0.1868 | Train Acc: 0.948 | Val Acc: 0.913\n",
      "Epoch [12/30] | Train Loss: 0.0912 | Val Loss: 0.1810 | Train Acc: 0.944 | Val Acc: 0.913\n",
      "Epoch [13/30] | Train Loss: 0.0863 | Val Loss: 0.2176 | Train Acc: 0.949 | Val Acc: 0.905\n",
      "Epoch [14/30] | Train Loss: 0.0822 | Val Loss: 0.2366 | Train Acc: 0.949 | Val Acc: 0.916\n",
      "Epoch [15/30] | Train Loss: 0.0764 | Val Loss: 0.1493 | Train Acc: 0.950 | Val Acc: 0.928\n",
      "Epoch [16/30] | Train Loss: 0.0705 | Val Loss: 0.1392 | Train Acc: 0.956 | Val Acc: 0.923\n",
      "Epoch [17/30] | Train Loss: 0.0764 | Val Loss: 0.1544 | Train Acc: 0.956 | Val Acc: 0.930\n",
      "Epoch [18/30] | Train Loss: 0.0725 | Val Loss: 0.1347 | Train Acc: 0.957 | Val Acc: 0.923\n",
      "Epoch [19/30] | Train Loss: 0.0651 | Val Loss: 0.1182 | Train Acc: 0.963 | Val Acc: 0.930\n",
      "Epoch [20/30] | Train Loss: 0.0623 | Val Loss: 0.1880 | Train Acc: 0.959 | Val Acc: 0.935\n",
      "Epoch [21/30] | Train Loss: 0.0718 | Val Loss: 0.1336 | Train Acc: 0.956 | Val Acc: 0.933\n",
      "Epoch [22/30] | Train Loss: 0.0581 | Val Loss: 0.1398 | Train Acc: 0.964 | Val Acc: 0.928\n",
      "Epoch [23/30] | Train Loss: 0.0572 | Val Loss: 0.1691 | Train Acc: 0.964 | Val Acc: 0.925\n",
      "Epoch [24/30] | Train Loss: 0.0512 | Val Loss: 0.1631 | Train Acc: 0.969 | Val Acc: 0.930\n",
      "Epoch [25/30] | Train Loss: 0.0515 | Val Loss: 0.1361 | Train Acc: 0.971 | Val Acc: 0.915\n",
      "Epoch [26/30] | Train Loss: 0.0497 | Val Loss: 0.1397 | Train Acc: 0.971 | Val Acc: 0.923\n",
      "Epoch [27/30] | Train Loss: 0.0497 | Val Loss: 0.1557 | Train Acc: 0.969 | Val Acc: 0.922\n",
      "Epoch [28/30] | Train Loss: 0.0448 | Val Loss: 0.1235 | Train Acc: 0.974 | Val Acc: 0.922\n",
      "Epoch [29/30] | Train Loss: 0.0511 | Val Loss: 0.1455 | Train Acc: 0.968 | Val Acc: 0.922\n",
      "Epoch [30/30] | Train Loss: 0.0438 | Val Loss: 0.1328 | Train Acc: 0.974 | Val Acc: 0.933\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss, train_correct, total = 0, 0, 0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_out, class_out = model(x_batch)\n",
    "\n",
    "        loss_recon = mse_loss(recon_out, x_batch)\n",
    "        loss_class = ce_loss(class_out, y_batch)\n",
    "        loss = loss_recon + 0.5 * loss_class# mix between losses to give more importance for each job that has higher loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(class_out, dim=1)\n",
    "        train_correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    train_acc = train_correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            recon_out, class_out = model(x_val)\n",
    "            loss_recon = mse_loss(recon_out, x_val)\n",
    "            loss_class = ce_loss(class_out, y_val)\n",
    "            loss = loss_recon + 0.5 * loss_class\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(class_out, dim=1)\n",
    "            val_correct += (preds == y_val).sum().item()\n",
    "            val_total += y_val.size(0)\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {train_loss/len(train_loader):.4f} | \"\n",
    "          f\"Val Loss: {val_loss/len(val_loader):.4f} | Train Acc: {train_acc:.3f} | Val Acc: {val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26ce13a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9560283687943263\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          S1       0.98      0.99      0.99       356\n",
      "          S2       0.91      0.85      0.87        91\n",
      "          S3       0.95      0.96      0.96       195\n",
      "          S4       0.89      0.89      0.89        63\n",
      "\n",
      "    accuracy                           0.96       705\n",
      "   macro avg       0.93      0.92      0.93       705\n",
      "weighted avg       0.96      0.96      0.96       705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        _, class_out = model(x_batch)\n",
    "        preds = torch.argmax(class_out, dim=1).cpu().numpy()\n",
    "        y_true.extend(y_batch.numpy())\n",
    "        y_pred.extend(preds)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbf2be16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and preprocessors saved.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"transformer_multi_task.pth\")\n",
    "joblib.dump(scaler, \"pose_scaler.pkl\")\n",
    "joblib.dump(label_encoder, \"phase_encoder.pkl\")\n",
    "print(\"Model and preprocessors saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
